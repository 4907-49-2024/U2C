%! Author = Alex
%! Date = 2025-03-18

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document
\begin{document}

    \section{Technical Section - NAME TBD}
    This section will go over the technical specifications of our program, as well as the process behind building it.
    \subsection{Program Architecture}
    The architecture of our program refers to the structures used to reason about our program, and how they relate to each other.

    \subsubsection{Architecture Choice}
    % Should these terms be defined here, or earlier?
    For our particular problem, we believed a pipe and filter architectural pattern could apply very well.
    The general pattern is as follows: Data comes from a \textbf{source},
    it is passed to a \textbf{pipe} which holds some immutable representation of that data while it remains in the pipe.
    From that point on, data from pipes are passed to \textbf{filters}.
    Filters take pipe data as input and outputs the transformed data to a different pipe.
    Thus, filter outputs can get chained as the next filter's inputs through pipes.
    Eventually, the final filter pipes its output to a \textbf{sink}, which represents the target data representation.\\

    From this point forward, we will refer to a \textbf{pipeline} as a function which takes a source as input,
    processes it with a predefined order of pipes and filters, then outputs a sink.
    We will also define \textbf{sub-pipelines} as pipelines created within another pipeline,
    taking in a pipe as their source, and output another pipe as their sink.
    They are useful for selecting alternate paths (choosing one sub-pipeline),
    or parallelizing a task which can split into independent steps (multiple instances of one pipeline with different inputs).\\


    Note, the distinction between a sink, a source, and a pipe can be nebulous.
    In practice, sources tend to be collections which can be processed by forking into different sub-pipelines.
    Conversely, sinks will then join these sub-pipeline outputs, with minimal modifications to the collected pipes.
    Further modifications should instead be done through another filter on this sink.
    In large pipelines with multiple depths of forking, these labels become confusing.
    Thus, the sinks/sources we will list refer only to the top level input/output of our program.
    That is, the inputs and outputs we mentioned in section~\ref{INTERFACE}\\

    This suitability of this architecture for our program becomes apparent when we start looking at its usual advantages and disadvantages.\\
    \textbf{Disadvantages:}
    \begin{itemize}
        \item Due to the independence of filters, pipe output/input need to be compatible.
        \item Difficult to coordinate, and synchronize.
        Unfit for asynchronous applications, like user interaction.
        \item Complex path branching logic difficult to implement due to limited information sharing.
    \end{itemize}
    \textbf{Advantages:}
    \begin{itemize}
        \item Loose coupling: can interchange filters with matching interfaces.
        \item Filters can be seen as black boxes, pipeline creation can ignore implementation of filters.
        \item Very easy to parallelize due to high independence.
        \item Re-usability: can re-use filters in different pipelines easily for different applications.
    \end{itemize}
    To put this in the context of our problem now, we have no user interaction in our system after the input is provided. We want to process our input diagrams, and eventually produce an output specification file. From this description, it already felt natural to try to make a pipeline, but there were many advantages from doing so. It opens the possibility to parallelize our processing easily, improving performance. The loose coupling also allows us to have redundant ways to compute certain pipes to include a model checker feature. Similarly, if we wanted to allow processing other diagram types as input, we could swap a subset of our current pipeline and re-use the rest. In general, it really helps for maintainability and extensibility of our program.

    \subsection{Main Structures}\label{subsec:main-structures}
    We will talk about the main structures of our architecture (pipes, filters, sinks, sources) in categories.
    This allows us to discuss what kind of filters could interchange in our existing filters
    rather than our current specific implementation.
    In section~\ref{subsec:main-pipeline}, we will map these categories to their concrete implementations in our current main pipeline.\\
    \textbf{Sources:}
    \begin{itemize}
        \item None (implicit) - In our prototype, to keep implementation simple the source options
        are hardcoded, as described in section~\ref{INPUTS}.
    \end{itemize}
    \textbf{Pipes:}
    \begin{itemize}
        \item ParserConfig: This pipe type configures data required to find the input, and how to read it.
        \item RawInput: The input as read in by our parser.
        \item Diagrams: Pipes to represent a diagram type, diagrams may be further broken down into their components.
        \item C2KA: Pipes related to specific C2KA concepts, like behaviors, stimuli, and individual next-mappings.
        \item Specifications: Collections of other C2KA concepts building up a particular C2KA specification.
    \end{itemize}
    \textbf{Filters:}
    \begin{itemize}
        \item InputParser: Takes in an InputConfig, returns it as a RawInput.
        \item DiagramLinker: Takes in a RawInput, and returns it as a Diagram.
        \item DiagramInterpreter: Takes in a Diagram, returns a C2KA Specification.
    \end{itemize}
    \textbf{Sinks:}
    \begin{itemize}
        \item C2KA Specifications: We currently have a single sink which takes in all specification types
        and provides a method to output them to a file.
    \end{itemize}

    \subsection{Main Pipeline}\label{subsec:main-pipeline}
    In section~\ref{subsec:main-structures} we discussed a high-level view of our components, and their connections.
    In this section, we will describe how we've implemented the main flow of our program through our main pipeline.
    \textbf{Forks} refer to steps where we instantiate a sub-pipeline.
    Conversely, \textbf{Joins} refer to steps where we collect outputs from multiple sub-pipelines.

    \begin{enumerate}
        \item Source (Implicit in pipeline): Read all files from hard-coded input folder.
        \item Fork: Instantiate one sub-pipeline per file passed in the source.
        \begin{enumerate}
            \item Pipe - XMIParserConfig: Takes in source file, and hard-coded state diagram configuration.
            \item Filter - XMIParser: Makes use of open source third-party XMI file reader SDMetrics.
            \item Pipe - ModelElement: SDMetrics representation of a generic model element.
            We specifically return the element representing the state diagram (which has references to all its sub-elements).
            \item Filter - StateDiagramLinker: Converts the SDMetrics representation to our internal representation of a state diagram.
            \item Pipe - SuperState: Generic representation of a superstate, has a name, contained states, transitions and regions.
            You may notice a state diagram also has all the above elements.
            We found that representing it the same way generalizes and simplifies interpretation later.
            \item Fork: Instantiate one sub-pipeline per interpreter filter type.
            \begin{itemize}
                \item Filter - StateAbstractBehaviorInterpreter: Reads the state diagram to output the AbstractBehaviorSpecification.
                \item Filter - StateConcreteBehaviorInterpreter: Reads the state diagram to output the ConcreteBehaviorSpecification.
                \item Filter - StateNextStimInterpreter: Reads the state diagram to output the NextStimulusSpecification.
                \item Filter - StateNextBehaviorInterpreter: Reads the state diagram to output the NextBehaviorSpecification.
            \end{itemize}
            \item Join: Combine all specifications from previous sub-pipelines into a C2KASpecification sink.
        \end{enumerate}
        \item Join: Output each sink to their output file.
        Note: This has to be done at the end (instead of asynchronously in sub-pipelines),
        because all system stimuli need to be known to fill the neutral outcomes of next mapping type specifications.
    \end{enumerate}

    Note that at this concrete level, a few assumptions are relied on, most notably that we only accept state diagrams.
    To extend our program to include collaboration diagrams like we originally planned,
    we would need to modify this pipeline for an alternate flow.
    Before step 2, we would insert a step to identify the diagram type.
    If the diagram is a state diagram, step 3A would be the same as the current step 2.
    If it is a collaboration diagram, step 3B would replace the current step 2 and instantiate a different sub-pipeline.
    The final step would be the same.
    We may create a new sink type for the new diagram,
    but it should implement a shared sink interface including a file output method.

\end{document}