%! Author = Alex
%! Date = 2025-03-18

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document
\begin{document}

    \section{Technical Section - NAME TBD}
    This section will go over the technical specifications of our program, as well as the process behind building it.
    \subsection{Program Architecture}
    The architecture of our program refers to the structures used to reason about our program, and how they relate to each other.

    \subsubsection{Architecture Choice}
    % Should these terms be defined here, or earlier?
    For our particular problem, we believed a pipe and filter architectural pattern could apply very well.
    The general pattern is as follows: Data comes from a \textbf{source},
    it is passed to a \textbf{pipe} which holds some immutable representation of that data while it remains in the pipe.
    From that point on, data from pipes are passed to \textbf{filters}.
    Filters take pipe data as input and outputs the transformed data to a different pipe.
    Thus, filter outputs can get chained as the next filter's inputs through pipes.
    Eventually, the final filter pipes its output to a \textbf{sink}, which represents the target data representation.\\

    From this point forward, we will refer to a \textbf{pipeline} as a function which takes a source as input,
    processes it with a predefined order of pipes and filters, then outputs a sink.
    We will also define \textbf{sub-pipelines} as pipelines created within another pipeline,
    taking in a pipe as their source, and output another pipe as their sink.
    They are useful for selecting alternate paths (choosing one sub-pipeline),
    or parallelizing a task which can split into independent steps (multiple instances of one pipeline with different inputs).\\


    Note, the distinction between a sink, a source, and a pipe can be nebulous.
    In practice, sources tend to be collections which can be processed by forking into different sub-pipelines.
    Conversely, sinks will then join these sub-pipeline outputs, with minimal modifications to the collected pipes.
    Further modifications should instead be done through another filter on this sink.
    In large pipelines with multiple depths of forking, these labels become confusing.
    Thus, the sinks/sources we will list refer only to the top level input/output of our program.
    That is, the inputs and outputs we mentioned in section~\ref{INTERFACE}\\

    This suitability of this architecture for our program becomes apparent when we start looking at its usual advantages and disadvantages.\\
    \textbf{Disadvantages:}
    \begin{itemize}
        \item Due to the independence of filters, pipe output/input need to be compatible.
        \item Difficult to coordinate, and synchronize.
        Unfit for asynchronous applications, like user interaction.
        \item Complex path branching logic difficult to implement due to limited information sharing.
    \end{itemize}
    \textbf{Advantages:}
    \begin{itemize}
        \item Loose coupling: can interchange filters with matching interfaces.
        \item Filters can be seen as black boxes, pipeline creation can ignore implementation of filters.
        \item Very easy to parallelize due to high independence.
        \item Re-usability: can re-use filters in different pipelines easily for different applications.
    \end{itemize}
    To put this in the context of our problem now, we have no user interaction in our system after the input is provided. We want to process our input diagrams, and eventually produce an output specification file. From this description, it already felt natural to try to make a pipeline, but there were many advantages from doing so. It opens the possibility to parallelize our processing easily, improving performance. The loose coupling also allows us to have redundant ways to compute certain pipes to include a model checker feature. Similarly, if we wanted to allow processing other diagram types as input, we could swap a subset of our current pipeline and re-use the rest. In general, it really helps for maintainability and extensibility of our program.

    \subsection{Main Structures}\label{subsec:main-structures}
    We will talk about the main structures of our architecture (pipes, filters, sinks, sources) in categories.
    This allows us to discuss what kind of filters could interchange in our existing filters
    rather than our current specific implementation.
    In section~\ref{subsec:main-pipeline}, we will map these categories to their concrete implementations in our current main pipeline.\\
    \textbf{Sources:}
    \begin{itemize}
        \item None (implicit) - In our prototype, to keep implementation simple the source options
        are hardcoded, as described in section~\ref{INPUTS}.
    \end{itemize}
    \textbf{Pipes:}
    \begin{itemize}
        \item ParserConfig: This pipe type configures data required to find the input, and how to read it.
        \item RawInput: The input as read in by our parser.
        \item Diagrams: Pipes to represent a diagram type, diagrams may be further broken down into their components.
        \item C2KA: Pipes related to specific C2KA concepts, like behaviors, stimuli, and individual next-mappings.
        \item Specifications: Collections of other C2KA concepts building up a particular C2KA specification.
    \end{itemize}
    \textbf{Filters:}
    \begin{itemize}
        \item InputParser: Takes in an InputConfig, returns it as a RawInput.
        \item DiagramLinker: Takes in a RawInput, and returns it as a Diagram.
        \item DiagramInterpreter: Takes in a Diagram, returns a C2KA Specification.
    \end{itemize}
    \textbf{Sinks:}
    \begin{itemize}
        \item C2KA Specifications: We currently have a single sink which takes in all specification types
        and provides a method to output them to a file.
    \end{itemize}

    \subsection{Main Pipeline}\label{subsec:main-pipeline}
    In section~\ref{subsec:main-structures} we discussed a high-level view of our components, and their connections.
    In this section, we will describe how we've implemented the main flow of our program through our main pipeline.
    \textbf{Forks} refer to steps where we instantiate a sub-pipeline.
    Conversely, \textbf{Joins} refer to steps where we collect outputs from multiple sub-pipelines.

    \begin{enumerate}
        \item Source (Implicit in pipeline): Read all files from hard-coded input folder.
        \item Fork: Instantiate one sub-pipeline per file passed in the source.
        \begin{enumerate}
            \item Pipe - XMIParserConfig: Takes in source file, and hard-coded state diagram configuration.
            \item Filter - XMIParser: Makes use of open source third-party XMI file reader SDMetrics.
            \item Pipe - ModelElement: SDMetrics representation of a generic model element.
            We specifically return the element representing the state diagram (which has references to all its sub-elements).
            \item Filter - StateDiagramLinker: Converts the SDMetrics representation to our internal representation of a state diagram.
            \item Pipe - SuperState: Generic representation of a superstate, has a name, contained states, transitions and regions.
            You may notice a state diagram also has all the above elements.
            We found that representing it the same way generalizes and simplifies interpretation later.
            \item Fork: Instantiate one sub-pipeline per interpreter filter type.
            \begin{itemize}
                \item Filter - StateAbstractBehaviorInterpreter: Reads the state diagram to output the AbstractBehaviorSpecification.
                \item Filter - StateConcreteBehaviorInterpreter: Reads the state diagram to output the ConcreteBehaviorSpecification.
                \item Filter - StateNextStimInterpreter: Reads the state diagram to output the NextStimulusSpecification.
                \item Filter - StateNextBehaviorInterpreter: Reads the state diagram to output the NextBehaviorSpecification.
            \end{itemize}
            \item Join: Combine all specifications from previous sub-pipelines into a C2KASpecification sink.
        \end{enumerate}
        \item Join: Output each sink to their output file.
        Note: This has to be done at the end (instead of asynchronously in sub-pipelines),
        because all system stimuli need to be known to fill the neutral outcomes of next mapping type specifications.
    \end{enumerate}

    Note that at this concrete level, a few assumptions are relied on, most notably that we only accept state diagrams.
    To extend our program to include collaboration diagrams like we originally planned,
    we would need to modify this pipeline for an alternate flow.
    Before step 2, we would insert a step to identify the diagram type.
    If the diagram is a state diagram, step 3A would be the same as the current step 2.
    If it is a collaboration diagram, step 3B would replace the current step 2 and instantiate a different sub-pipeline.
    The final step would be the same.
    We may create a new sink type for the new diagram,
    but it should implement a shared sink interface including a file output method.

    \section{Reflections}
    talk about deviations and stuff

    \section{Conclusion}
    We wanted to find a way to produce C2KA system specifications in an easier way.
    C2KA specifications are a useful formalism to analyze systems, but they take time and knowledge to produce.
    As such, other methods to gain assurance in a system are used.
    These are typically lower in cost, but cannot provide the same degree of assurance (like unit testing code).\\

    We believed that we could leverage a more universal skill than C2KA specification writing:
    creating visual diagram representations of the system.
    Visual diagrams have the added benefit of being artefacts useful on their own already.
    This means users of our tool may already be using them, or benefit from their creation.\\

    The prototype we've built takes in UML state diagrams and can produce C2KA specification files for each agent in the system.
    On their own, they can provide a comprehensive overview of the possible behaviors of an agent in the system.
    When fed to other analysis tools like the IIAT,
    these specifications can also be used to analyze the system for implicit interactions.
    These implicit interactions may be the source of hidden vulnerabilities in the system,
    which we can now help detect easily.

    \subsection{Further Recommendations}
    There are a few gaps in our prototype compared to our requirements.
    For an up-to-date detailed account, our GitHub issues should be the best source in case of any changes after this report was written.
    For consistency, we will follow the issue categories we used in GitHub to discuss our recommendations.
    See section~\ref{} for precise definitions on these categories.

    \subsubsection{Verification \& Validation}
    We are lacking a formal definition of coverage criteria.
    The test strategy we defined does a good job at capturing the idea behind how we try to cover tests.
    Formalizing coverage criteria would be taking these ideas and formulating them in a way that we can systematically
    test our code coverage, and build a report to see if we conform to our desired coverage.
    This report would also be an important baseline for assurance in our software.
    Checking coverage is done manually at the moment, we make issues if we notice missing coverage (easy to miss). \\

    We also lack system test (validation tests) variety.
    It would have been nice to have system tests to test other metrics than ``accuracy'' of our system.
    We have no stress tests for performance for example.
    This is partially due to a weak definition of our non-functional requirements, as mentioned in \ref{REQ MANAGEMENT}. \\
    % (TODO: I want to justify this in the requirements section, but it is important to talk about its drawbacks here)

    In general, more tests.
    We are currently lacking coverage (verification).
    More system tests increase assurance in our system (validation), we have very few of them at the moment.

    \subsubsection{Enhancements}
    The main feature we are lacking is collaboration diagram processing.
    We cannot produce an intended sequence of inputs for the IIAT, which was part of our initial set of functional requirements.

    Another notable mention was performance improvement related fixes.
    Our testing did not yet support proper analysis for these changes, thus work on them did not get started either.

    We entertained the idea of having a separate model checker module as part of our tool.
    We could have a separate pipeline that could be called independently if desired to check the validity of the inputs.
    We would define rules in the module to check the assumptions we defined in \ref{INPUTS}.
    This would help prevent potential unexpected behaviors (false positive outputs),
    and it could help improve the error messages we give to users to make fixing user diagrams easier.
    The side benefit is by trying to make the artefacts work for our tool,
    we may improve diagram quality by automatically reviewing them through our model checker.


    \subsubsection{Bugs}
    There are none (that we know of!).

    In this context, we will count bugs as faults which could lead to an incorrect output being produced.
    If the program crashes before producing the output, we may count it as a missing feature depending on the cause.
    A well-formed input which crashes means we have not developed the tools to process it yet.
    Collaboration diagrams could fall in this category, although we have not yet defined what is a well-formed collaboration diagram either.
    Otherwise, it is good if the program crashes because it means that it detected a fault in the input.
    If it did not, it would be a bug because the output is faulty (false positive output).

    Our philosophy as we were finishing our prototype was to focus on fully functional essential features over many potentially broken features.
    The hope is that we may not be able to output everything, but what we do produce can be relied on.


\end{document}