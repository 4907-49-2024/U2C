%! Author = Alex
%! Date = 2025-03-18

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

\usepackage{longtable}
\usepackage[table]{xcolor}
\usepackage{tabularx} % Package to manage table width automatically

% Document
\begin{document}
    \section{Technical Details}\label{sec:technical-details}
    \subsection{Usage}\label{subsec:usage}

    \subsection{System Development}

    \label{subsec:system-development}
    % Change to User requirements?
    \subsubsection{User Requirements}\label{subsubsec:user-reqs}

    These User Requirements represent our original set of requirements, with minimal implementation details, and domain knowledge.
    They are elicited from initial discussions with our supervisor,
    and interpretations of the problem statement.
    We did not establish a procedure for explicit lower level requirements after this stage.

    Some limitations of this process is a lack of explicit detail, priorities, or levels of satisfaction for non-functional requirements.
    Instead, we will use later stages of the system development to trace design decisions taken for these requirements.
    We will also touch on priorities and possible improvements when we explain
    which requirements were skipped for our current release in section~\ref{subsubsec:unsat-reqs}.

    The table of requirements contains a list of User Requirements for our system,
    with green states indicating satisfactory completion, and gray indicating unsatisfactory:
    % Requirement table
    \begin{longtable}{|l|p{2.6cm}|l|p{4.5cm}|c|}
        \hline
        \textbf{ID} & \textbf{Requirement} & \textbf{Type}  & \textbf{Description} & \textbf{State}\\
        \hline
        \endhead
        \hline
        % Requirements here
        1 & Input & Functional & U2C shall accept visual system models as input. & \cellcolor{green!30}  \\
        \hline
        2 & C2KA Specifications & Functional & U2C shall interpret given inputs to derive a complete C2KA agent specification. & \cellcolor{green!30}  \\
        \hline
        3 & IIAT Parameters & Functional & U2C shall interpret given inputs to derive the other required IIAT inputs. & \cellcolor{gray!30}  \\
        \hline
        4 & Output & Functional & U2C shall output its computed inputs as text files. & \cellcolor{green!30}  \\
        \hline
        5 & Minimal User Actions & Usability & U2C shall require no additional inputs from the user apart from those required to provide system models. & \cellcolor{green!30}  \\
        \hline
        6 & Modelling Tool Support & Compatibility & U2C shall support models produced by at least one chosen modelling tool (Papyrus, LucidChart, StarUML). & \cellcolor{green!30}  \\
        \hline
        7 & Modelling Language Support & Compatibility & U2C shall support visual models drawn in at least one type of modelling language (ex: UML, SysML, BPMN). & \cellcolor{green!30}  \\
        \hline
        8 & Diagram Type Support & Compatibility & U2C shall support diagrams corresponding to at least one chosen type (ex: state, collaboration, etc.). & \cellcolor{green!30}  \\
        \hline
        9 & Cross Tool Integration & Compatibility & U2C should provide options to integrate directly into tools it depends on (input producer, and output consumers). & \cellcolor{gray!30}  \\
        \hline
        10 & OS Support & Portability & U2C shall run on at least one chosen OS distribution. & \cellcolor{green!30}  \\
        \hline
        11 & Simple System Analysis Speed & Performance & U2C shall produce outputs for a simple system within one minute of execution on a chosen platform specification.
        A simple system is defined as having up to five agents, twenty stimuli, and five behaviors per agent. & \cellcolor{gray!30}  \\
        \hline
        12 & Worst Case Mitigation & Scalability & U2C should scale at a smaller rate than O($n^3$). The expected scaling factors are agents, stimuli, and agent behaviors. & \cellcolor{gray!30}  \\
        \hline
        13 & User Documentation & Maintainability & The U2C code repository shall contain a user manual detailing expected user interactions, and input formats. & \cellcolor{gray!30}  \\
        \hline
        14 & Maintainer Documentation & Maintainability & The U2C code repository shall contain documentation detailing implementation details for maintainers. & \cellcolor{green!30}  \\
        \hline
        15 & Verification Testing & Maintainability & The U2C code repository shall contain automated regression tests providing adequate coverage of the program source code. & \cellcolor{green!30}  \\
        \hline
        16 & Accurate Outputs & Reliability & U2C shall provide deterministic outputs representing accurately what the inputs contain. & \cellcolor{green!30}  \\
        \hline
        17 & No False Positives & Reliability & U2C shall not provide outputs if it cannot find a deterministic interpretation of the given inputs. & \cellcolor{green!30}  \\
        \hline
    \end{longtable}
    %TODO add caption?

    \subsubsection{Chosen Inputs}
    The first decision to take concerning inputs was the specific diagram type to support.
    This decision impacts the language, since the diagram type may be specific to one or a small set of languages.
    It also impacts the modelling tool because it needs to support creating that diagram type.
    Most importantly, if the diagram does not have the information we require, there is no purpose in processing it.
    
    Recall that the information we require is related to creating C2KA agent specifications.
    Our initial diagram choice was \textit{collaboration diagrams}, due to their simplicity. 
    When we learned more about C2KA, we realized they do not model agent behavior very well.
    Collaboration diagrams are more suited to describe a specific scenario on an entire system.
    Instead, we found that having one \textbf{UML State Diagram} per agent is enough to simply capture agent specifications.
    More details on this later when we break down relevant components of state diagrams in section~\ref{subsubsec:input-specification}.

    By choosing UML State Diagrams, we also choose \textbf{UML} as our modelling language.
    SysML could have been a close viable alternative, but we decided to choose a language we were more familiar with.
    This makes it easier to implement properly for us,
    but it is also related to our problem statement aiming to use modelling languages already familiar to system designers to reduce the learning curve.
    Admittedly outside of software system design SysML is more prevalent, and it is feasible to support both, but we had to limit our scope.

    For our modelling tool, we chose \textbf{Papyrus}.
    We established evaluation criteria to evaluate different tools, and compared the most popular ones we found.
    The most important requirement was machine readability.
    We needed to be able to export some representation of the diagram our system could read outside the modelling tool.
    Additionally, it was an advantage if it was free, popular, and had ongoing support.
    We did not want the modelling tool we depended on to become a hurdle to adopt our program,
    ideally we chose a tool which was already in use.
    Other tools we looked at were lucidchart, draw.io, creately, gliffy, UMLLet, PlantUML (all had poor export functions),
    Modelio (felt hard to use), Astah UML (unpopular),
    StarUML and Visual Paradigm (seemed like good alternatives, but we figured out how to use Papyrus first).

    To export diagrams from Papyrus, there is an option to export \textbf{XMI files}.
    They technically have a .uml file extension in papyrus, but they function the same as XMI files from other tools.
    Once a diagram for an agent is finished in papyrus, we can add its XMI file to our program's \textbf{input folder}.
    Once all agents are done, we can execute the program.
    The user does not need to provide additional inputs.
    A full workflow using our program is described in section~\ref{subsec:usage}.

    The table below is a traceability summary mapping our original user requirements to the concepts refining them.
    Requirements are shown in the order their refinements appear in this section.
    Refinements can be correlated by the bolded terms above.

    \begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{| l | l | X |}
        % Header
        \hline
        \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
%        \hline
%        \endhead
        % Header end
        \hline
        8 & Diagram Type Support & The chosen input diagram type is UML State Diagrams. \\ \hline
        7 & Modelling Language Support & The chosen modelling language is UML. \\ \hline
        6 & Modelling Tool Support & The chosen modelling tool is Papyrus. \\ \hline
        1 & Input & Visual Diagrams are given to the system through XMI file exports from supported modelling tool(s). \\ \hline
        5 & Minimal User Actions & The user puts files in an input folder, then executes the program.  \\ \hline
    \end{tabularx}\label{tab:input-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \subsubsection{Input Diagram Specification}\label{subsubsec:input-specification}
    Although we accept UML State diagrams, there are specific elements that our interpreter cares about.
    The primary goal was to make use of essential model elements only, specifically states and transitions,
    to avoid extra modelling requirements.
    Unfortunately, modelling expressiveness was a smaller priority, as such
    other elements may cause unexpected behavior.
    Ideally, they are ignored.
    The next best case is a descriptive error gets raised and crashes the program preventing a faulty output.
    In the worst case, an inaccurate specification may be generated without warning.

    For the safest behavior, we have defined a known set of valid model elements, along with usage guidelines.
    At a high level, it can be summarized with the following class diagram:
    \\
    % TODO: Insert supported state elems figure here

    The first element in our state diagram is a \textbf{state}.
    States describe a discrete behavior where an invariant condition holds.
    They map directly to \textit{Abstract Behaviors} in C2KA\@.
    \textbf{Atomic States} are states which do not contain other states.
    % TODO: (add C2KA base figures after each paragraph?)

    Atomic States always have a \textbf{doActivity} property,
    as they are needed for C2KA \textit{Concrete Behaviors}.
    doActivities need to follow Dijkstra's Guarded Command Language (GCL) notation.
    They are used to assign environmental shared variable values,
    with the option to use conditional flows based on the current environment state.
    Due to typesetting constraints, we had to convert some GCL operands.
    The full list of supported operands and their GCL notation equivalent is listed in the table below:
    \\
    % TODO: Add GCL table

    \textbf{Transitions} are links between states which describe how changes in state occur.
    We use these transitions and their associated behaviors to compute \textit{Next Behavior} and \textit{Next Stimulus} functions.
    For C2KA, we have defined a precise labelling format to follow \{input-stimulus\}/\{output-stimulus\}.
    This follows state diagram convention, but it does not allow for guards,
    and enforces input and output on all transitions.
    \textit{sequential transitions} are special transitions with a single stimulus, their labelling format is: \{in-and-out-stimulus\}.
    This is because sequential states use the output of one state as the input of the next state in the sequence.
    This allows us to uniquely identify sequential compositions by using a single stimulus as the key identifying property.

    \textbf{Sequential States} are super states (state compositions) with a specific composition pattern.
    There are at least two inner states.
    All inner states are linked by \textit{sequential transitions}.
    The sequence has no cycles.
    There is a clear initial state with no incoming transitions.
    There is a clear final state with no outgoing transitions.

    Note: We explicitly decided not cover C2KA \textit{parallel compositions} to simplify modelling and parsing.
    Instead, a modeller should consider making two different agents which communicate
    with each other to model parallel behavior.

    \subsubsection{XMI Parsing}\label{subsubsec:parsing}
    % TODO: look at overlap between this and technical background
    Having chosen our input format, we needed to find a way to parse it to be able to process it.
    Although XMI is supposed to be a standardized format,
    many modelling tools will have slight differences for specific components.
    Therefore, creating a parser which can be extended generally to support many tools can be challenging.

    Thankfully, this is a general problem others have attempted to solve before, and third-party free libraries exist.
    We searched for libraries which supported the most recent UML standards, and we were able to make work.
    The libraries we tested were \textit{SDMetrics OpenCore},
    \textit{eclipse UML2}, \textit{Apache Xerces}, \textit{XMIParser, by cqframework}, \textit{xmiparser, python}.
    We only managed to get the \textbf{SDMetrics} Java library to work within our research period.
    Since it adequately met our needs, we did not extend the research period to find other alternatives.

    The table below traces user requirements to the parsing choices refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            1 & Input & Use SD Metrics OpenCore library to parse XMI file inputs. \\ \hline
        \end{tabularx}\label{tab:parse-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?
    % Table mapping Diagram Elem, Purpose, C2KA Map
%    \begin{table}[htbp]
%        \centering
%        \begin{tabularx}{\textwidth}{| p{2cm} | X | p{2cm} |}
%            \hline
%            % Header
%            \textbf{Diagram Element} & \textbf{Modelling Purpose} & \textbf{C2KA Concept} \\ \hline
%            % Header end
%            State & Describes a discrete behavior where an invariant condition holds & Abstract Behavior \\ \hline
%            Super State & Used to show groups of related inner states, we specifically use them to show sequentially linked behaviors & \textbf{Sequential Behavior} \\ \hline
%            Transition & Represents inputs causing state changes, and outputs between state changes & Input and Output stimuli \\ \hline
%            Do Activity & Concrete Program executed while in state (Using \textbf{Dijkstra's GCL notation}) & Concrete Behavior \\ \hline
%        \end{tabularx}\label{tab:diagram-element-table}
%    \end{table}

    \newpage
    \subsubsection{Target Deployment}
    One of our user requirements requires us to target an Operating System.
    Just like the choice of modelling tool, we want to avoid the OS target to be a barrier to our tool.
    We also want to avoid incurring costs attempting to port our tool across multiple Operating Systems.
    \textbf{Java} is a great language which allows us to run the same program on any OS due to the Java Virtual Machine.

    We considered \textit{C}, but it is not as simple to make platform independent.
    \textit{Python} was also a valid alternative because it is also platform independent.
    The deciding factor was the availability of third party libraries.
    As mentioned in section~\ref{subsubsec:parsing} The SD Metrics library we use for XMI parsing in our system was implemented in Java.
    We were not able to make any alternatives work in Python.
    With no particular need for other languages, we decided to keep it simple by only using Java in our program.

    The table below traces user requirements to the deployment choices refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            10 & OS Support & Use java for platform independence. \\ \hline
        \end{tabularx}\label{tab:os-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \newpage
    \subsubsection{Architecture Choice}
    Choosing how to structure our code depends strongly on our functional requirements,
    but quality attributes can play a role as well.
    The most relevant quality attributes for the architecture for are scalability and performance.
    The other requirements are addressed at different steps of the design process.

    To begin with, we can think about what our program does not need, like user interaction during execution.
    This already gets rid of the need for user interaction focused patterns like \textit{Model-View-Controller} and its variants.
    We also have no need for a server or decentralized processing unless we failed to meet our performance goal on one computer.
    We did not expect performance to be an issue big enough to require decentralized processing.
    Thus, we can eliminate any server patterns, including \textit{microservice} and \textit{service oriented} architectures.

    The initial architecture we chose was the \textit{Layer Pattern}, because it fits quite well with the flow of our program.
    We are provided an input, and do a series of unidirectional transformations on it to produce an output at the end.
    We believed these transformations made sense as different layers of our program.
    Layers could pass their outputs through defined input interfaces of the next layer.
    This is great for separation of concerns and maintainability.
    It is also a great way to reason about the system allowing us to convert our understanding of C2KA transformation to code easily.

    That said, we then realized we were starting to describe a \textbf{Pipe and Filter} architecture.
    Instead of layers to transform data, we use filters.
    We use pipes as our interface to communicate between them.
    We still have great separation of concerns, and the same straightforward reasoning for implementation.
%    In fact, we can still informally categorize filters into layers for organization as we will see in section~\ref{subsubsec:arch-desc}
    The advantage we gain from this pivot is the ability to increase parallelization of processing compared to the layered architecture.
    This allows us to directly improve our \textbf{performance} and \textbf{scalability} by reducing the impact of increased data set sizes.

    The table below traces user requirements to the architecture choices refining them in this section.

    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            11 & Simple System Analysis Speed & Use parallelization enabled by Pipe and Filter to improve performance. \\ \hline
            12 & Worst Case Mitigation & Use parallelization enabled by Pipe and Filter to reduce scaling costs.  \\ \hline
        \end{tabularx}\label{tab:arch-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \newpage
    \subsubsection{Architecture Description}\label{subsubsec:arch-desc}
    To describe the concrete implementation of our design,
    we will break down the responsibilities of our filters, and the pipes in our system through a visual representation.
    In these representations, the filters are the named rounded rectangles.
    The directional associations show data flow between filters.
    Annotations on the data flows specify what data is contained in the pipe.
    We also have forks, which are flows going into a black vertical bar and splitting, showing when data is re-used in parallel.
    Joins are the opposite operation, collecting data from independent parallel operations back into one centralized point.
    Finally, we have the cloud to indicate a simplification in the diagram.
    It explains which details were omitted from the model to make it easier to understand.

    The first part of our architecture model focuses on converting our expected input to a StateDiagram internally.
    % TODO: Model P1 here
    When the program starts execution, it reads all the model files at once from a known input address.
    It forks once for each model to process them all in parallel.
    The first transformation step is to parse the \textit{XMI file} with our \textbf{XMI Parser}.
    The parser uses the SDMetrics library to extract generic UML \textit{ModelElements}.

    These elements need to be passed to our \textbf{State Diagram Linker}
    to build an internal representation of a \textit{state diagram} for our program.
    Specifically, we strongly type model elements according to our internally defined types (states, and transitions).
    Super states can be thought of as tree roots, and a state diagram can also be represented as a super state.
    This allows us to recursively interpret our diagram just like a tree during the following \textbf{diagram interpretation} stage.

    The second part of our model completes the process, going from internal StateDiagram to the desired output.
    % TODO Model P2
    From our \textit{state diagram}, we can perform different analyses in parallel to find parts of the C2KA specification.
    The \textbf{abstract behavior interpreter} looks at super states to find sequential compositions,
    and atomic state names to build a C2KA \textit{abstract behavior specification}.
    States which are not sequentially linked are composed by choice.

    The \textbf{concrete behavior interpreter} simply collects all the atomic behaviors and their doActivities.
    The doActivities should already be formatted as the concrete behavior according to our input specification (section~\ref{subsubsec:input-specification}).
    The C2KA \textit{concrete behavior specification} produced is just a formatted collection of the doActivities of the agent.

    The \textbf{next behavior interpreter} checks transitions to find next behavior mappings.
    A mapping is an initial behavior, an input stimulus, and the resulting behavior of the transition.
    The \textit{next behavior specification} is the set of all next behavior mappings found for that agent.

    The \textbf{next stimulus interpreter} is essentially the same as next behavior,
    but instead of the resulting behavior, we check the output stimulus on the transition.
    The \textit{next stimulus specification} is the set of all next stimulus mappings found for that agent.

    Once all the interpretation is complete, it gets joined into a complete \textit{C2KA Specification}.
    We then need to wait at a barrier for all other models to complete analysis before writing our final \textit{output} to a file.

    The reason for the barrier is due to a limitation inherent in C2KA of the next stimulus, and next behavior functions.
    They need to be complete functions,
    meaning behaviors in an agent need to have defined neutral mappings for all stimuli in the system.
    Neutral mappings being outcomes where nothing changes, a neutral stimulus is sent or the behavior remains the same.
    However, since the view of our system is fragmented across our input set,
    we cannot identify all stimuli in the system until we analyze all agent inputs to get a complete view of our system.


    The table below traces user requirements to the architecture implementations refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            1 & Input & XMI Parser, and State Diagram Linker used to convert input to a diagram format we can interpret. \\ \hline
            2 & C2KA Specifications & Interpreter filters are used to convert a diagram to partial C2KA specifications.
            They are then combined into a full C2KA specification. \\ \hline
            4 & Output & The complete C2KA Specification to output conversion allows us to extract agent text files as output.  \\ \hline
        \end{tabularx}\label{tab:arch-description-table}
    \end{table}

    \newpage
    \subsubsection{Project Management}\label{subsubsec:proj-mngmnt}
    After user requirements, we described our design choices to cover them.
    From design, we needed to create tasks to trace the design to specific code implementation.
    To do so, we used \textbf{GitHub Issues} to build a list of upcoming tasks for the project.
    This allowed us to assign them when resources were available, and communicate task dependencies easily.
    Work done by individuals can be tracked through issues,
    preventing multiple people from working on a task simultaneously unaware.
    The transition from weekly meetings to asynchronous communication through issues improved our efficiency immensely.
    % TODO sample issue list

    We also customized issue tags to categorize issues
    according to our major development concerns, as seen in our repository:
    % TODO: Insert issue types
    \textit{Enhancements} and \textit{bugs} both relate to implementing our design,
    but bugs are errors we found after merging the initial enhancement implementation.
    \textit{Verification \& Validation} (V\&V) Tasks related to verifying our code, and validating it.
    Usually through creating new tests, but it can also include creating tools for testing,
    or establishing and documenting V\&V requirements or strategies.

    \textit{Questions} are typically related to clarifications on requirements,
    or designs which require research or input from our supervisor.
    Once a question is answered, we typically reply directly in the question issue and close it.
    Usually, questions have a purpose and can create new issues or unblock existing ones as well.
    \textit{Documentation} Tasks related to documenting our project, for any audience.
    This includes documentation like this report, function descriptions in code, and a user manual.

    \textit{Out of scope} Marks tasks which we identified as out of scope for our current release.
    These can include requirements we create to explicitly exclude from the start,
    or existing issues we drop later due to time constraints.
    In the case where an issue is deemed irrelevant, we do not mark it out of scope.
    Instead, we close the issue with a comment for rationale and stop tracking it.
    On release, we reset the scope constraints by removing this label on all issues.
    After an evaluation of the goals of the next release,
    maintainers can decide which issues should stay within the scope of the next release.

    We did not develop a great way to automatically trace issues to design,
    and we have too many issues to go through them all (over 60 closed issues currently).
    Unfortunately, this means we cannot rely on design to code traceability.
    Instead, we rely only on testing for validation.
    We can, however, demonstrate how we could attempt to manually show traceability through a specific issue.
    %TODO add specific issue, explain

    \newpage
    \subsubsection{Testing Strategy}\label{subsubsec:tests-strat}
    Although we never defined a formal test selection criteria,
    we defined informal guidelines to define how to verify our code.

    For our simplest code unit, \textit{pipes}, we can \textbf{unit test} it.
    After construction, we verify public attributes are as expected.

    For \textit{filters}, we did \textbf{integration tests} to more closely simulate a real program environment.
    For these tests, we called all the filters up to and including the filter under test then evaluated the output.
    This allowed us to do iterative development when we developed new filters,
    ensuring new filter implementations were not breaking any previously implemented filters.
    The following diagram shows how an integration test would like when testing the State Diagram Linker.
    % TODO Make a diagram to show a test on XMI parser

    For integration test inputs, we defined a group of elementary C2KA representations which implement
    all the elements specified as part of our input specification.
    The diagrams used are the same diagrams included in the input specification section~\ref{subsubsec:input-specification}.

    For the complete \textit{C2KASpecification} we did end to end \textbf{system tests}, with a \textbf{custom diff tool}.
    To do this, we needed a system with known C2KA outputs.
    We got analyzed systems from our supervisor, which we treated as the source of truth.
    Then, we also needed to model diagrams representing those specifications.
    Finally, we create a test with the diagrams as inputs, the analyzed system as expected outputs,
    and compare them with our diff tool.

    The reason we needed a custom diff tool is due to the inevitable differences between manual C2KA analysis,
    and our automated analysis.
    There may be some whitespace differences or a different ordering of lines
    which are both semantically irrelevant to the specification.
    This is why our diff tool checks for an exact match for lines in a specification type,
    regardless of their location within the block or any whitespace.
    This means that if our actual produced output is missing a line, the diff raises an error,
    but it also raises one if our output has an extra line which is not in the expected output.

    % TODO show output differences

    The table below traces user requirements to our testing strategy concepts refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            15 & Verification Testing & Unit tests, Integration tests provided confidence that our code was continuously functional as we progressed. \\ \hline
            16 & Accurate Outputs & The system tests and diff tool were part of our validation activities to verify output accuracy. \\ \hline
            17 & No False Positives & The system tests should show failures if an output could not be completed, otherwise the diff tool should raise an error. \\ \hline
        \end{tabularx}\label{tab:test-strat-table}
    \end{table}
    \newpage
    \subsubsection{Validation Results}\label{subsubsec:test-validation}


    \newpage
    \subsubsection{Unsatisfied Requirements} \label{subsubsec:unsat-reqs}
    % Remove / rework later
    Out of Scope
    \begin{enumerate}
        \item Cross tool integration
        \item IIAT Parameters
        \item ~Modelling Tool Support
        \item ~Modelling Language Support
        \item ~Diagram Type Support
    \end{enumerate}
    Missing Tests
    \begin{enumerate}
        \item Simple System Analysis Speed
        \item Worst Case Mitigation
        \item Verification of our diff tool
        \item ~Verification tests
        \item ~Accurate Outputs
        \item ~False Positives
    \end{enumerate}
    Missing Docs % TODO: this could get completed before the report
    \begin{enumerate}
        \item User docs
        \item ~(cover above?) Maintainer docs
    \end{enumerate}
    % TODO: talk possible improvements on quality attributes

    \section{Conclusion}\label{sec:conclusion}
    \input{sections/conclusion}

\end{document}