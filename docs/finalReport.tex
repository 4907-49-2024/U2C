%! Author = Alex
%! Date = 2025-03-18

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

\usepackage{longtable}
\usepackage[table]{xcolor}
\usepackage{tabularx} % Package to manage table width automatically

% Document
\begin{document}
    \section{Technical Details}\label{sec:technical-details}
    \subsection{Usage}\label{subsec:usage}

    \subsection{System Development}

    \label{subsec:system-development}
    % Change to User requirements?
    \subsubsection{User Requirements}

    These User Requirements represent our original set of requirements, with minimal implementation details, and domain knowledge.
    They are elicited from initial discussions with our supervisor,
    and interpretations of the problem statement.
    We did not establish a procedure for explicit lower level requirements after this stage.

    Some limitations of this process is a lack of explicit detail, priorities, or levels of satisfaction for non-functional requirements.
    Instead, we will use later stages of the system development to trace design decisions taken for these requirements.
    We will also touch on priorities and possible improvements when we explain
    which requirements were skipped for our current release in section~\ref{subsubsec:unsat-reqs}.

    The table of requirements contains a list of User Requirements for our system,
    with green states indicating satisfactory completion, and gray indicating unsatisfactory:
    % Requirement table
    \begin{longtable}{|l|p{2.6cm}|l|p{4.5cm}|c|}
        \hline
        \textbf{ID} & \textbf{Requirement} & \textbf{Type}  & \textbf{Description} & \textbf{State}\\
        \hline
        \endfirsthead
        \hline
        \textbf{ID} & \textbf{Requirement} & \textbf{Type}  & \textbf{Description} & \textbf{State}\\
        \hline
        \endhead
        \hline
        % Requirements here
        1 & Input & Functional & U2C shall accept visual system models as input. & \cellcolor{green!30}  \\
        \hline
        2 & C2KA Specifications & Functional & U2C shall interpret given inputs to derive a complete C2KA agent specification. & \cellcolor{green!30}  \\
        \hline
        3 & IIAT Parameters & Functional & U2C shall interpret given inputs to derive the other required IIAT inputs. & \cellcolor{gray!30}  \\
        \hline
        4 & Output & Functional & U2C shall output its computed inputs as text files. & \cellcolor{green!30}  \\
        \hline
        5 & Minimal User Actions & Usability & U2C shall require no additional inputs from the user apart from those required to provide system models. & \cellcolor{green!30}  \\
        \hline
        6 & Modelling Tool Support & Compatibility & U2C shall support models produced by at least one chosen modelling tool (Papyrus, LucidChart, StarUML). & \cellcolor{green!30}  \\
        \hline
        7 & Modelling Language Support & Compatibility & U2C shall support visual models drawn in at least one type of modelling language (ex: UML, SysML, BPMN). & \cellcolor{green!30}  \\
        \hline
        8 & Diagram Type Support & Compatibility & U2C shall support diagrams corresponding to at least one chosen type (ex: state, collaboration, etc.). & \cellcolor{green!30}  \\
        \hline
        9 & Cross Tool Integration & Compatibility & U2C should provide options to integrate directly into tools it depends on (input producer, and output consumers). & \cellcolor{gray!30}  \\
        \hline
        10 & OS Support & Portability & U2C shall run on at least one chosen OS distribution. & \cellcolor{green!30}  \\
        \hline
        11 & Simple System Analysis Speed & Performance & U2C shall produce outputs for a simple system within one minute of execution on a chosen platform specification.
        A simple system is defined as having up to five agents, twenty stimuli, and five behaviors per agent. & \cellcolor{gray!30}  \\
        \hline
        12 & Worst Case Mitigation & Scalability & U2C should scale at a smaller rate than O($n^3$). The expected scaling factors are agents, stimuli, and agent behaviors. & \cellcolor{gray!30}  \\
        \hline
        13 & User Documentation & Maintainability & The U2C code repository shall contain a user manual detailing expected user interactions, and input formats. & \cellcolor{gray!30}  \\
        \hline
        14 & Maintainer Documentation & Maintainability & The U2C code repository shall contain documentation detailing implementation details for maintainers. & \cellcolor{green!30}  \\
        \hline
        15 & Verification Testing & The U2C code repository shall contain automated regression tests providing adequate coverage of the program source code. & \cellcolor{gray!30}  \\
        \hline
        16 & Accurate Outputs & Reliability & U2C shall provide deterministic outputs representing accurately what the inputs contain. & \cellcolor{green!30}  \\
        \hline
        17 & No False Positives & Reliability & U2C shall not provide outputs if it cannot find a deterministic interpretation of the given inputs. & \cellcolor{green!30}  \\
        \hline
    \end{longtable}
    %TODO add caption?

    \subsubsection{Chosen Inputs}
    The first decision to take concerning inputs was the specific diagram type to support.
    This decision impacts the language, since the diagram type may be specific to one or a small set of languages.
    It also impacts the modelling tool because it needs to support creating that diagram type.
    Most importantly, if the diagram does not have the information we require, there is no purpose in processing it.
    Recall that the information we require is related to creating C2KA agent specifications.
    It turns out, it is possible to make one \textbf{UML State Diagram} per agent to simply capture its complete specification.
    More details on this later when we break down relevant components of state diagrams in section~\ref{subsubsec:input-example}.

    By choosing UML State Diagrams, we also choose \textbf{UML} as our modelling language.
    SysML could have been a close viable alternative, but we decided to choose a language we were more familiar with.
    This makes it easier to implement properly for us,
    but it is also related to our problem statement aiming to use modelling languages already familiar to system designers to reduce the learning curve.
    Admittedly outside of software system design SysML is more prevalent, and it is feasible to support both, but we had to limit our scope.

    For our modelling tool, we chose \textbf{Papyrus}.
    We established evaluation criteria to evaluate different tools, and compared the most popular ones we found.
    The most important requirement was machine readability.
    We needed to be able to export some representation of the diagram our system could read outside the modelling tool.
    Additionally, it was an advantage if it was free, popular, and had ongoing support.
    We did not want the modelling tool we depended on to become a hurdle to adopt our program,
    ideally we chose a tool which was already in use.
    Other tools we looked at were lucidchart, draw.io, creately, gliffy, UMLLet, PlantUML (all had poor export functions),
    Modelio (felt hard to use), Astah UML (unpopular),
    StarUML and Visual Paradigm (seemed like good alternatives, but we figured out how to use Papyrus first).

    To export diagrams from Papyrus, there is an option to export \textbf{XMI files}.
    They technically have a .uml file extension in papyrus, but they function the same as XMI files from other tools.
    Once a diagram for an agent is finished in papyrus, we can add its XMI file to our program's \textbf{input folder}.
    Once all agents are done, we can execute the program.
    The user does not need to provide additional inputs.
    A full workflow using our program is described in section~\ref{subsec:usage}.

    The table below is a traceability summary mapping our original user requirements to the concepts refining them.
    Requirements are shown in the order their refinements appear in this section.
    Refinements can be correlated by the bolded terms above.

    \begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{| l | l | X |}
        % Header
        \hline
        \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
%        \hline
%        \endhead
        % Header end
        \hline
        8 & Diagram Type Support & The chosen input diagram type is UML State Diagrams. \\ \hline
        7 & Modelling Language Support & The chosen modelling language is UML. \\ \hline
        6 & Modelling Tool Support & The chosen modelling tool is Papyrus. \\ \hline
        1 & Input & Visual Diagrams are given to the system through XMI file exports from supported modelling tool(s). \\ \hline
        5 & Minimal User Actions & The user puts files in an input folder, then executes the program.  \\ \hline
    \end{tabularx}\label{tab:input-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \subsubsection{Well-Formed Diagrams}\label{subsubsec:input-example}
    Although we accept UML State diagrams, there are specific elements that our interpreter cares about.
    The primary goal was to make use of essential model elements only, specifically states and transitions,
    to avoid extra modelling requirements.
    Unfortunately, modelling expressiveness was a smaller priority, as such
    other elements may cause unexpected behavior.
    Ideally, they are ignored.
    The next best case is a descriptive error gets raised and crashes the program preventing a faulty output.
    In the worst case, an inaccurate specification may be generated without warning.

    For the safest behavior, we have defined a known set of valid model elements, along with usage guidelines.
    At a high level, it can be summarized with the following class diagram:
    \\
    % TODO: Insert supported state elems figure here

    The first element in our state diagram is a \textbf{state}.
    States describe a discrete behavior where an invariant condition holds.
    They map directly to \textit{Abstract Behaviors} in C2KA\@.
    \textbf{Atomic States} are states which do not contain other states.
    % TODO: (add C2KA base figures after each paragraph?)

    Atomic States always have a \textbf{doActivity} property,
    as they are needed for C2KA \textit{Concrete Behaviors}.
    doActivities need to follow Dijkstra's Guarded Command Language (GCL) notation.
    They are used to assign environmental shared variable values,
    with the option to use conditional flows based on the current environment state.
    Due to typesetting constraints, we had to convert some GCL operands.
    The full list of supported operands and their GCL notation equivalent is listed in the table below:
    \\
    % TODO: Add GCL table

    \textbf{Transitions} are links between states which describe how changes in state occur.
    We use these transitions and their associated behaviors to compute \textit{Next Behavior} and \textit{Next Stimuli} functions.
    For C2KA, we have defined a precise labelling format to follow \{input-stimuli\}/\{output-stimuli\}.
    This follows state diagram convention, but it does not allow for guards,
    and enforces input and output on all transitions.
    \textit{sequential transitions} are special transitions with a single stimuli, their labelling format is: \{in-and-out-stimuli\}.
    This is because sequential states use the output of one state as the input of the next state in the sequence.
    This allows us to uniquely identify sequential compositions by using a single stimuli as the key identifying property.

    \textbf{Sequential States} are super states (state compositions) with a specific composition pattern.
    There are at least two inner states.
    All inner states are linked by \textit{sequential transitions}.
    The sequence has no cycles.
    There is a clear initial state with no incoming transitions.
    There is a clear final state with no outgoing transitions.

    \subsubsection{XMI Parsing}\label{subsubsec:parsing}
    % TODO: look at overlap between this and technical background
    Having chosen our input format, we needed to find a way to parse it to be able to process it.
    Although XMI is supposed to be a standardized format,
    many modelling tools will have slight differences for specific components.
    Therefore, creating a parser which can be extended generally to support many tools can be challenging.

    Thankfully, this is a general problem others have attempted to solve before, and third-party free libraries exist.
    We searched for libraries which supported the most recent UML standards, and we were able to make work.
    The libraries we tested were \textit{SDMetrics OpenCore},
    \textit{eclipse UML2}, \textit{Apache Xerces}, \textit{XMIParser, by cqframework}, \textit{xmiparser, python}.
    We only managed to get the \textbf{SDMetrics} Java library to work within our research period.
    Since it adequately met our needs, we did not extend the research period to find other alternatives.

    The table below traces user requirements to the parsing choices refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            1 & Input & Use SD Metrics OpenCore library to parse XMI file inputs. \\ \hline
        \end{tabularx}\label{tab:parse-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?
    % Table mapping Diagram Elem, Purpose, C2KA Map
%    \begin{table}[htbp]
%        \centering
%        \begin{tabularx}{\textwidth}{| p{2cm} | X | p{2cm} |}
%            \hline
%            % Header
%            \textbf{Diagram Element} & \textbf{Modelling Purpose} & \textbf{C2KA Concept} \\ \hline
%            % Header end
%            State & Describes a discrete behavior where an invariant condition holds & Abstract Behavior \\ \hline
%            Super State & Used to show groups of related inner states, we specifically use them to show sequentially linked behaviors & \textbf{Sequential Behavior} \\ \hline
%            Transition & Represents inputs causing state changes, and outputs between state changes & Input and Output stimuli \\ \hline
%            Do Activity & Concrete Program executed while in state (Using \textbf{Dijkstra's GCL notation}) & Concrete Behavior \\ \hline
%        \end{tabularx}\label{tab:diagram-element-table}
%    \end{table}

    \newpage
    \subsubsection{Target Deployment}
    One of our user requirements requires us to target an Operating System.
    Just like the choice of modelling tool, we want to avoid the OS target to be a barrier to our tool.
    We also want to avoid incurring costs attempting to port our tool across multiple Operating Systems.
    \textbf{Java} is a great language which allows us to run the same program on any OS due to the Java Virtual Machine.

    We considered \textit{C}, but it is not as simple to make platform independent.
    \textit{Python} was also a valid alternative because it is also platform independent.
    The deciding factor was the availability of third party libraries.
    As mentioned in section~\ref{subsubsec:parsing} The SD Metrics library we use for XMI parsing in our system was implemented in Java.
    We were not able to make any alternatives work in Python.
    With no particular need for other languages, we decided to keep it simple by only using Java in our program.

    The table below traces user requirements to the deployment choices refining them in this section.
    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            10 & OS Support & Use java for platform independence. \\ \hline
        \end{tabularx}\label{tab:os-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \newpage
    \subsubsection{Architecture Choice}
    Choosing how to structure our code depends strongly on our functional requirements,
    but quality attributes can play a role as well.
    The most relevant quality attributes for the architecture for are scalability and performance.
    The other requirements are addressed at different steps of the design process.

    To begin with, we can think about what our program does not need, like user interaction during execution.
    This already gets rid of the need for user interaction focused patterns like \textit{Model-View-Controller} and its variants.
    We also have no need for a server or decentralized processing unless we failed to meet our performance goal on one computer.
    We did not expect performance to be an issue big enough to require decentralized processing.
    Thus, we can eliminate any server patterns, including \textit{microservice} and \textit{service oriented} architectures.

    The initial architecture we chose was the \textit{Layer Pattern}, because it fits quite well with the flow of our program.
    We are provided an input, and do a series of unidirectional transformations on it to produce an output at the end.
    We believed these transformations made sense as different layers of our program.
    Layers could pass their outputs through defined input interfaces of the next layer.
    This is great for separation of concerns and maintainability.
    It is also a great way to reason about the system allowing us to convert our understanding of C2KA transformation to code easily.

    That said, we then realized we were starting to describe a \textbf{Pipe and Filter} architecture.
    Instead of layers to transform data, we use filters.
    We use pipes as our interface to communicate between them.
    We still have great separation of concerns, and the same straightforward reasoning for implementation.
%    In fact, we can still informally categorize filters into layers for organization as we will see in section~\ref{subsubsec:arch-desc}
    The advantage we gain from this pivot is the ability to increase parallelization of processing compared to the layered architecture.
    This allows us to directly improve our \textbf{performance} and \textbf{scalability} by reducing the impact of increased data set sizes.

    The table below traces user requirements to the architecture choices refining them in this section.

    \begin{table}[htbp]
        \centering
        \begin{tabularx}{\textwidth}{| l | l | X |}
            % Header
            \hline
            \textbf{ID} & \textbf{User Requirement} & \textbf{Refinement} \\
            % Header end
            \hline
            11 & Simple System Analysis Speed & Use parallelization enabled by Pipe and Filter to improve performance. \\ \hline
            12 & Worst Case Mitigation & Use parallelization enabled by Pipe and Filter to reduce scaling costs.  \\ \hline
        \end{tabularx}\label{tab:arch-choice-table}
    \end{table}
    % TODO table caption?... how to, should we?

    \newpage
    \subsubsection{Architecture Description}\label{subsubsec:arch-desc}

    \newpage
    \subsubsection{Project Management}\label{subsubsec:proj-mngmnt}
    After user requirements, we described our design choices to cover them.
    From design, we needed to create tasks to trace the design to specific code implementation.
    To do so, we used \textbf{GitHub Issues} to build a list of upcoming tasks for the project.
    This allowed us to assign them when resources were available, and communicate task dependencies easily.
    Work done by individuals can be tracked through issues,
    preventing multiple people from working on a task simultaneously unaware.
    The transition from weekly meetings to asynchronous communication through issues improved our efficiency immensely.
    % TODO sample issue list

    We also customized issue tags to categorize issues
    according to our major development concerns, as seen in our repository:
    % TODO: Insert issue types
    \textit{Enhancements} and \textit{bugs} both relate to implementing our design,
    but bugs are errors we found after merging the initial enhancement implementation.
    \textit{Verification \& Validation} (V\&V) Tasks related to verifying our code, and validating it.
    Usually through creating new tests, but it can also include creating tools for testing,
    or establishing and documenting V\&V requirements or strategies.

    \textit{Questions} are typically related to clarifications on requirements,
    or designs which require research or input from our supervisor.
    Once a question is answered, we typically reply directly in the question issue and close it.
    Usually, questions have a purpose and can create new issues or unblock existing ones as well.
    \textit{Documentation} Tasks related to documenting our project, for any audience.
    This includes documentation like this report, function descriptions in code, and a user manual.

    \textit{Out of scope} Marks tasks which we identified as out of scope for our current release.
    These can include requirements we create to explicitly exclude from the start,
    or existing issues we drop later due to time constraints.
    In the case where an issue is deemed irrelevant, we do not mark it out of scope.
    Instead, we close the issue with a comment for rationale and stop tracking it.
    On release, we reset the scope constraints by removing this label on all issues.
    After an evaluation of the goals of the next release,
    maintainers can decide which issues should stay within the scope of the next release.

    We did not develop a great way to automatically trace issues to design,
    and we have too many issues to go through them all (over 60 closed issues currently).
    Unfortunately, this means we cannot rely on design to code traceability.
    Instead, we rely only on testing for validation.
    We can, however, demonstrate how we could attempt to manually show traceability through a specific issue.
    %TODO add specific issue, explain

    \newpage
    \subsubsection{Testing Strategy}\label{subsubsec:tests-strat}


    \newpage
    \subsubsection{Validation Results}\label{subsubsec:test-validation}


    \newpage
    \subsubsection{Unsatisfied Requirements} \label{subsubsec:unsat-reqs}
    % Remove / rework later
    Out of Scope
    \begin{enumerate}
        \item Cross tool integration
        \item IIAT Parameters
        \item ~Modelling Tool Support
        \item ~Modelling Language Support
        \item ~Diagram Type Support
    \end{enumerate}
    Missing Tests
    \begin{enumerate}
        \item Simple System Analysis Speed
        \item Worst Case Mitigation
        \item ~Verification tests
        \item ~Accurate Outputs
        \item ~False Positives
    \end{enumerate}
    Missing Docs % TODO: this could get completed before the report
    \begin{enumerate}
        \item User docs
        \item ~(cover above?) Maintainer docs
    \end{enumerate}
    % TODO: talk possible improvements on quality attributes





    \section{Technical Section - NAME TBD}
    This section will go over the technical specifications of our program, as well as the process behind building it.
    \subsection{Program Architecture}

    % FIXME: The tiered approach is nice, do it progressively on the real figure though. Leave pipes implicit
    \subsubsection{Main Structures}\label{subsec:main-structures}
    We will talk about the main structures of our architecture (pipes, filters, sinks, sources) in categories.
    This allows us to discuss what kind of filters could interchange in our existing filters
    rather than our current specific implementation.
    In section~\ref{subsec:main-pipeline}, we will map these categories to their concrete implementations in our current main pipeline.\\
    \textbf{Sources:}
    \begin{itemize}
        \item None (implicit) - In our prototype, to keep implementation simple the source options
        are hardcoded, as described in section~\ref{INPUTS}.
    \end{itemize}
    \textbf{Pipes:}
    \begin{itemize}
        \item ParserConfig: This pipe type configures data required to find the input, and how to read it.
        \item RawInput: The input as read in by our parser.
        \item Diagrams: Pipes to represent a diagram type, diagrams may be further broken down into their components.
        \item C2KA: Pipes related to specific C2KA concepts, like behaviors, stimuli, and individual next-mappings.
        \item Specifications: Collections of other C2KA concepts building up a particular C2KA specification.
    \end{itemize}
    \textbf{Filters:}
    \begin{itemize}a
        \item InputParser: Takes in an InputConfig, returns it as a RawInput.
        \item DiagramLinker: Takes in a RawInput, and returns it as a Diagram.
        \item DiagramInterpreter: Takes in a Diagram, returns a C2KA Specification.
    \end{itemize}
    \textbf{Sinks:}
    \begin{itemize}
        \item C2KA Specifications: We currently have a single sink which takes in all specification types
        and provides a method to output them to a file.
    \end{itemize}

    \subsection{Main Pipeline}\label{subsec:main-pipeline}
    In section~\ref{subsec:main-structures} we discussed a high-level view of our components, and their connections.
    In this section, we will describe how we've implemented the main flow of our program through our main pipeline.
    \textbf{Forks} refer to steps where we instantiate a sub-pipeline.
    Conversely, \textbf{Joins} refer to steps where we collect outputs from multiple sub-pipelines.

    \begin{enumerate}
        \item Source (Implicit in pipeline): Read all files from hard-coded input folder.
        \item Fork: Instantiate one sub-pipeline per file passed in the source.
        \begin{enumerate}
            \item Pipe - XMIParserConfig: Takes in soasdurce file, and hard-coded state diagram configuration.
            \item Filter - XMIParser: Makes use of open source third-party XMI file reader SDMetrics.
            \item Pipe - ModelElement: SDMetrics representation of a generic model element.
            We specifically return the element representing the state diagram (which has references to all its sub-elements).
            \item Filter - StateDiagramLinker: Converts the SDMetrics representation to our internal representation of a state diagram.
            \item Pipe - SuperState: Generic representation of a superstate, has a name, contained states, transitions and regions.
            You may notice a state diagram also has all the above elements.
            We found that representing it the same way generalizes and simplifies interpretation later.
            \item Fork: Instantiate one sub-pipeline per interpreter filter type.
            \begin{itemize}
                \item Filter - StateAbstractBehaviorInterpreter: Reads the state diagram to output the AbstractBehaviorSpecification.
                \item Filter - StateConcreteBehaviorInterpreter: Reads the state diagram to output the ConcreteBehaviorSpecification.
                \item Filter - StateNextStimInterpreter: Reads the state diagram to output the NextStimulusSpecification.
                \item Filter - StateNextBehaviorInterpreter: Reads the state diagram to output the NextBehaviorSpecification.
            \end{itemize}
            \item Join: Combine all specifications from previous sub-pipelines into a C2KASpecification sink.
        \end{enumerate}
        \item Join: Output each sink to their output file.
        Note: This has to be done at the end (instead of asynchronously in sub-pipelines),
        because all system stimuli need to be known to fill the neutral outcomes of next mapping type specifications.
    \end{enumerate}

    % FIXME: I don't like this part that much, but maybe we can still talk about how the presented arch can be extended still?
    Note that at this concrete level, a few assumptions are relied on, most notably that we only accept state diagrams.
    To extend our program to include collaboration diagrams like we originally planned,
    we would need to modify this pipeline for an alternate flow.
    Before step 2, we would insert a step to identify the diagram type.
    If the diagram is a state diagram, step 3A would be the same as the current step 2.
    If it is a collaboration diagram, step 3B would replace the current step 2 and instantiate a different sub-pipeline.
    The final step would be the same.
    We may create a new sink type for the new diagram,
    but it should implement a shared sink interface including a file output method.

    \section{Reflections}
    \subsection{Requirement Elicitation Phase}
    This section refers to how we collected background information, and determined our requirements from there.
    This does not refer to how we managed requirements after we started the implementation.
    The issue with this phase is two-fold.
    We took way too much time (we did not start implementation work until after the progress report),
    and it was not productive given the time spent in it.\\

    At multiple times during implementation, progress halted due to a lack of C2KA knowledge,
    or we had to rewrite code from a poor initial understanding.
    We also did not put any effort into approving and tracing any requirements other than our functional objectives.
    These functional objectives were based on a specific pipeline configuration which we decided to change later,
    and thus were too low-level to base our functional requirements on them.
    The non-functional objectives of our system became even more implicit, and were subject to change based on
    since there was no traceability for them.
    This means any decisions regarding trade-offs in the program were decided based off of what felt best at the time,
    not an objective metric measured against non-functional requirements.\\

    Our requirement elicitation should have been more targeted, and produced a Software Requirement Specification document
    that we verified, and agreed to conform to.
    We should have made a process for ongoing requirement elicitation during development.
    In the case of a knowledge gap being identified, we would research or ask questions to fill this gap,
    then produce requirements and update our SRS accordingly.
    This turnaround should not take more than a week to avoid blocking or reverting changes in the implementation.
    The initial requirement elicitation phase may warrant a bit more time allocated.
    It takes time to develop this process and learn about the problem domain.
    It should not have taken us more than a month,
    it is better to fail and identify knowledge gaps earlier than be as inefficient as we were.

    \subsection{Timelines}
    There were a few problems with the initial timeline.
    The first one was keeping an optional objective and excluding it from our timeline.
    Our timeline represented the worst case scenario to still deliver a prototype.
    This meant our target of a high-quality project required us to outperform the timeline we proposed.\\

    We also did not include any slack-time in our project, we included it by allocating extra time instead.
    This meant components that had six weeks allocated
    to them were really supposed to be implemented in potentially a week or two in normal circumstances.
    This, and keeping the optional objective implicit made us quite lax in the early stages of the project.\\

    The timeline was also based on two assumptions which did not hold and made it impossible to achieve.
    We had agreed as a team to have

    %

    \section{Conclusion}
    We wanted to find a way to produce C2KA system specifications in an easier way.
    C2KA specifications are a useful formalism to analyze systems, but they take time and knowledge to produce.
    As such, other methods to gain assurance in a system are used.
    These are typically lower in cost, but cannot provide the same degree of assurance (like unit testing code).\\

    We believed that we could leverage a more universal skill than C2KA specification writing:
    creating visual diagram representations of the system.
    Visual diagrams have the added benefit of being artefacts useful on their own already.
    This means users of our tool may already be using them, or benefit from their creation.\\

    The prototype we've built takes in UML state diagrams and can produce C2KA specification files for each agent in the system.
    On their own, they can provide a comprehensive overview of the possible behaviors of an agent in the system.
    When fed to other analysis tools like the IIAT,
    these specifications can also be used to analyze the system for implicit interactions.
    These implicit interactions may be the source of hidden vulnerabilities in the system,
    which we can now help detect easily.

    \subsection{Further Recommendations}
    There are a few gaps in our prototype compared to our requirements.
    For an up-to-date detailed account, our GitHub issues should be the best source in case of any changes after this report was written.
    For consistency, we will follow the issue categories we used in GitHub to discuss our recommendations.
    See section~\ref{} for precise definitions on these categories.

    \subsubsection{Verification \& Validation}
    We are lacking a formal definition of coverage criteria.
    The test strategy we defined does a good job at capturing the idea behind how we try to cover tests.
    Formalizing coverage criteria would be taking these ideas and formulating them in a way that we can systematically
    test our code coverage, and build a report to see if we conform to our desired coverage.
    This report would also be an important baseline for assurance in our software.
    Checking coverage is done manually at the moment, we make issues if we notice missing coverage (easy to miss). \\

    We also lack system test (validation tests) variety.
    It would have been nice to have system tests to test other metrics than ``accuracy'' of our system.
    We have no stress tests for performance for example.
    This is partially due to a weak definition of our non-functional requirements, as mentioned in \ref{REQ MANAGEMENT}. \\
    % (TODO: I want to justify this in the requirements section, but it is important to talk about its drawbacks here)

    In general, more tests.
    We are currently lacking coverage (verification).
    More system tests increase assurance in our system (validation), we have very few of them at the moment.

    \subsubsection{Enhancements}
    The main feature we are lacking is collaboration diagram processing.
    We cannot produce an intended sequence of inputs for the IIAT, which was part of our initial set of functional requirements.

    Another notable mention was performance improvement related fixes.
    Our testing did not yet support proper analysis for these changes, thus work on them did not get started either.

    We entertained the idea of having a separate model checker module as part of our tool.
    We could have a separate pipeline that could be called independently if desired to check the validity of the inputs.
    We would define rules in the module to check the assumptions we defined in \ref{INPUTS}.
    This would help prevent potential unexpected behaviors (false positive outputs),
    and it could help improve the error messages we give to users to make fixing user diagrams easier.
    The side benefit is by trying to make the artefacts work for our tool,
    we may improve diagram quality by automatically reviewing them through our model checker.


    \subsubsection{Bugs}
    There are none (that we know of!).

    In this context, we will count bugs as faults which could lead to an incorrect output being produced.
    If the program crashes before producing the output, we may count it as a missing feature depending on the cause.
    A well-formed input which crashes means we have not developed the tools to process it yet.
    Collaboration diagrams could fall in this category, although we have not yet defined what is a well-formed collaboration diagram either.
    Otherwise, it is good if the program crashes because it means that it detected a fault in the input.
    If it did not, it would be a bug because the output is faulty (false positive output).

    Our philosophy as we were finishing our prototype was to focus on fully functional essential features over many potentially broken features.
    The hope is that we may not be able to output everything, but what we do produce can be relied on.


\end{document}